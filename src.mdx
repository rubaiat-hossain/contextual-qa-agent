# Monitoring AI Agents with Helicone

As AI agents become more common in apps and services, it's essential to understand how they behave. In this guide, we'll build a multi-step qa analysis agent powered by Groq for fast LLM responses and wrap it with a REST API so you can easily use it using tools like Curl. Then, we'll integrate Helicone to monitor and debug the agent in real-time, giving you a clear view of how it performs and responds.

## Setting Up the QA Analysis Agent

Our simple AI agent can answer questions about specific topics by retrieving information from its RAG knowledge base, or handle general statements with contextual awareness. It also routes all LLM traffic through Helicone, which acts as a proxy and observability layer, letting us track requests, analyze metrics latencies, debug sessions, and more.

You can get the full working agent at this [GitHub repository](https://github.com/rubaiat-hossain/contextual-qa-agent).

### Prerequisites

To follow along, you’ll need the following tools and API keys configured.

* Node.js v18+
* Npm - installed and running
* ChromaDB - running on port 8000
* Groq API key – https://console.groq.com
* Helicone API key – https://www.helicone.ai

### Step 1. Initialize the project

Begin by creating a new directory for your project and initializing it using the following commands.

```bash
mkdir qa-agent-tutorial
cd qa-agent-tutorial
npm init -y
```

### Step 2. Install the dependencies

You need to install the necessary packages for our AI agent using these commands.

```bash
npm install express dotenv zod chromadb groq-sdk @helicone/helpers
npm install -D typescript tsx @types/node @types/express
```

### Step 3. Configure TypeScript

Initialize TypeScript configuration using -

```bash
npx tsc --init
```

### Step 4. Create your agent

Create a `main.ts` file and add the following code to it.

```ts
import { config } from "dotenv";
config();

import express, { Request, Response } from "express";
import Groq from "groq-sdk";
import { randomUUID } from "crypto";
// @ts-ignore
import { ChromaClient } from "chromadb";
import { HeliconeManualLogger } from "@helicone/helpers";

const app = express();
app.use(express.json());

const chroma = new ChromaClient({ path: "http://localhost:8000" });
let collection: Awaited<ReturnType<ChromaClient["getOrCreateCollection"]>>;

const groq = new Groq({
  apiKey: process.env.GROQ_API_KEY ?? "",
  baseURL: "https://groq.helicone.ai",
  defaultHeaders: {
    "Helicone-Auth": `Bearer ${process.env.HELICONE_API_KEY ?? ""}`,
  },
});

const heliconeLogger = new HeliconeManualLogger({
  apiKey: process.env.HELICONE_API_KEY,
});

async function getCurrentTime(sessionId: string, sessionName: string): Promise<string> {
  try {
    const startTime = Date.now();
    const now = new Date().toISOString();
    const timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
    const localTime = new Date().toLocaleString();
    
    const toolOutput = {
      timestamp: now,
      metadata: {
        timezone: timezone,
        localTime: localTime,
        unix_timestamp: Math.floor(Date.now() / 1000),
        tool_version: "1.0.3",
        execution_duration_ms: Date.now() - startTime
      }
    };
    
    await groq.chat.completions.create(
      {
        messages: [
          {
            role: "user",
            content: `Tool: getCurrentTime\nInput: {}\nOutput: ${JSON.stringify(toolOutput, null, 2)}`,
          },
        ],
        model: "meta-llama/llama-4-scout-17b-16e-instruct",
        temperature: 0,
        max_tokens: 5,
      },
      {
        headers: {
          "Helicone-Session-Id": sessionId,
          "Helicone-Session-Path": "/manual-tool-log",
          "Helicone-Session-Name": sessionName,
          "Helicone-Property-ToolName": "getCurrentTime",
          "Helicone-Property-ToolType": "function",
          "Helicone-Property-QueryType": "time-request",
          "Helicone-Property-ToolInfo": "ISO timestamp generator",
          "Helicone-Property-ExecutionTime": now,
          "Helicone-Property-ExecutionDuration": `${Date.now() - startTime}ms`,
          "Helicone-Property-Timezone": timezone,
          "Helicone-Property-ToolVersion": "1.0.3"
        },
      }
    );
    
    return now;
  } catch (error) {
    console.error("Error in getCurrentTime tool:", error);
    throw error;
  }
}

async function retrieveFromChromaDB(query: string, sessionId: string, sessionName: string): Promise<string> {
  try {
    const startTime = Date.now();
    
    const results = await collection.query({
      queryTexts: [query],
      nResults: 3,
    });
    
    const matches = results.documents?.[0] || [];
    const distances = results.distances?.[0] || [];
    const ids = results.ids?.[0] || [];
    
    const document = matches[0] ?? "No relevant knowledge found.";
    
    const toolOutput = {
      document: document,
      metadata: {
        query: query,
        matches_count: matches.length,
        top_matches: matches.map((doc, i) => ({
          id: ids[i],
          text: doc,
          relevance_score: distances[i] ? (1 - distances[i]) : 0
        })),
        execution_duration_ms: Date.now() - startTime,
        db_name: "knowledge",
        tool_version: "1.0.3"
      }
    };
    
    await groq.chat.completions.create(
      {
        messages: [
          {
            role: "user",
            content: `Tool: ChromaDBRetrieval\nInput: ${JSON.stringify({ query })}\nOutput: ${JSON.stringify(toolOutput, null, 2)}`,
          },
        ],
        model: "meta-llama/llama-4-scout-17b-16e-instruct",
        temperature: 0,
        max_tokens: 5,
      },
      {
        headers: {
          "Helicone-Session-Id": sessionId,
          "Helicone-Session-Path": "/knowledge-retrieval",
          "Helicone-Session-Name": sessionName,
          "Helicone-Property-ToolName": "ChromaDBRetrieval",
          "Helicone-Property-ToolType": "retrieval",
          "Helicone-Property-QueryType": "knowledge-request",
          "Helicone-Property-ToolInfo": "Vector database lookup",
          "Helicone-Property-ExecutionTime": new Date().toISOString(),
          "Helicone-Property-ExecutionDuration": `${Date.now() - startTime}ms`,
          "Helicone-Property-MatchCount": matches.length.toString(),
          "Helicone-Property-DBName": "knowledge",
          "Helicone-Property-ToolVersion": "1.0.3"
        },
      }
    );
    
    return document;
  } catch (error) {
    console.error("Error in ChromaDBRetrieval tool:", error);
    throw error;
  }
}

async function seedKnowledgeBase() {
  collection = await chroma.getOrCreateCollection({ name: "knowledge" });

  const count = await collection.count();
  if (count === 0) {
    await collection.add({
      ids: ["ai", "helicone", "rag", "observability", "mcp", "llmops"],
      metadatas: [
        { topic: "AI" },
        { topic: "Helicone" },
        { topic: "RAG" },
        { topic: "Observability" },
        { topic: "Model Context Protocol" },
        { topic: "LLMOps" },
      ],
      documents: [
        "Artificial Intelligence (AI) is the simulation of human-like intelligence by machines to perform tasks like learning, reasoning, and problem-solving.",
        "Helicone is a developer platform for monitoring and debugging AI agents and LLM applications. It provides observability into prompts, costs, latency, and session flows.",
        "Retrieval-Augmented Generation (RAG) is an AI framework that retrieves external information and injects it into the prompt for better, more factual responses.",
        "Observability in AI refers to the practice of understanding, monitoring, and debugging model behavior by tracking key metrics like latency, cost, token usage, and session traces.",
        "Model Context Protocol (MCP) allows AI agents to connect external tools, APIs, or services dynamically at runtime using a standard protocol for enhanced capabilities.",
        "LLMOps refers to operational practices around managing, monitoring, scaling, and debugging large language models in production environments.",
      ],
    });
  }
}

async function processMultiStepQuery(text: string): Promise<string> {
  const startTime = Date.now();
  const sessionId = randomUUID();
  const runId = `run_${randomUUID().substring(0, 8)}`;
  const sessionName = "Multi-Step RAG Agent";
  const isTimeQuestion = /\b(what\s+time\s+is\s+it|current\s+time|time\s+now)\b/i.test(text);
  
  let context = "";
  let stepsTaken = [];

  if (isTimeQuestion) {
    stepsTaken.push("time_detection");
    const time = await getCurrentTime(sessionId, sessionName);
    context = `Current time is ${time}`;
    stepsTaken.push("time_retrieval");
  } else {
    stepsTaken.push("query_classification");
    
    const classify = await groq.chat.completions.create(
      {
        messages: [
          { role: "system", content: "Respond ONLY with 'question' or 'general'." },
          { role: "user", content: `Classify: "${text}"` },
        ],
        model: "meta-llama/llama-4-scout-17b-16e-instruct",
        temperature: 0.3,
        max_tokens: 10,
      },
      {
        headers: {
          "Helicone-Session-Id": sessionId,
          "Helicone-Session-Path": "/classify",
          "Helicone-Session-Name": sessionName,
          "Helicone-Property-Step": "classification",
          "Helicone-Property-QueryText": text.substring(0, 100),
          "Helicone-Property-RunId": runId,
          "Helicone-Property-StepNumber": "1"
        },
      }
    );

    const classification = classify.choices?.[0]?.message?.content?.trim().toLowerCase() ?? "general";
    
    if (classification.includes("question")) {
      stepsTaken.push("knowledge_retrieval");
      context = await retrieveFromChromaDB(text, sessionId, sessionName);
    } else {
      stepsTaken.push("time_fallback");
      const time = await getCurrentTime(sessionId, sessionName);
      context = `Current time is ${time}`;
    }
  }

  stepsTaken.push("reasoning");
  
  const reasoning = await groq.chat.completions.create(
    {
      messages: [
        { role: "system", content: "You are a reasoning assistant." },
        { role: "user", content: `Context: "${context}". Reason about how to answer "${text}".` },
      ],
      model: "meta-llama/llama-4-scout-17b-16e-instruct",
      temperature: 0.3,
      max_tokens: 200,
    },
    {
      headers: {
        "Helicone-Session-Id": sessionId,
        "Helicone-Session-Path": "/reasoning",
        "Helicone-Session-Name": sessionName,
        "Helicone-Property-Step": "reasoning",
        "Helicone-Property-Context": context.substring(0, 100),
        "Helicone-Property-RunId": runId,
        "Helicone-Property-StepNumber": "2"
      },
    }
  );

  const reasoningOutput = reasoning.choices?.[0]?.message?.content ?? "";
  stepsTaken.push("response_generation");
  
  const finalReply = await groq.chat.completions.create(
    {
      messages: [
        {
          role: "system",
          content: `Friendly assistant.\nContext: "${context}".\nReasoning: "${reasoningOutput}".`,
        },
        { role: "user", content: text },
      ],
      model: "meta-llama/llama-4-scout-17b-16e-instruct",
      temperature: 0.5,
      max_tokens: 500,
    },
    {
      headers: {
        "Helicone-Session-Id": sessionId,
        "Helicone-Session-Path": "/final-response",
        "Helicone-Session-Name": sessionName,
        "Helicone-Property-Step": "response",
        "Helicone-Property-QueryType": isTimeQuestion ? "time" : "knowledge",
        "Helicone-Property-RunId": runId,
        "Helicone-Property-StepNumber": "3",
        "Helicone-Property-ExecutionPath": stepsTaken.join(","),
        "Helicone-Property-TotalDuration": `${Date.now() - startTime}ms`
      },
    }
  );

  return finalReply.choices?.[0]?.message?.content ?? "No final response.";
}

app.post("/analyze", async (req: Request, res: Response) => {
  const { text } = req.body;
  if (!text) {
    res.status(400).json({ error: "Missing text" });
    return;
  }

  try {
    const startTime = Date.now();
    const result = await processMultiStepQuery(text);
    const duration = Date.now() - startTime;
    
    res.json({ 
      response: result,
      metadata: {
        processing_time_ms: duration,
        timestamp: new Date().toISOString(),
        query_length: text.length
      }
    });
  } catch (err: unknown) {
    console.error("Error:", err);
    res.status(500).json({ error: err instanceof Error ? err.message : "Internal Server Error" });
  }
});

app.get("/health", (_req: Request, res: Response) => {
  res.json({ 
    status: "ok", 
    timestamp: new Date().toISOString(),
    uptime: process.uptime()
  });
});

const PORT = process.env.PORT || 3000;
seedKnowledgeBase().then(() => {
  app.listen(PORT, () => {
    console.log(`Server running at http://localhost:${PORT}`);
  });
});
```

### Step 5. Configure environment

Create a `.env` file and add your API keys.

```bash
GROQ_API_KEY=your_groq_api_key
HELICONE_API_KEY=your_helicone_api_key
```
You also need to start ChromaDB.

```bash
docker run -p 8000:8000 ghcr.io/chroma-core/chroma:latest
```

### Step 6. Run the agent

With your environment variables configured, you can now start the agent.

```bash
npx tsx main.ts
```

If you've set up the environment correctly, you'll see -

`Server running at http://localhost:3000`

### Step 7. Test the agent

Once our AI agent is running, it exposes a REST endpoint that you can use to send queries and inspect its behavior.

You can test the agent using `curl` -

```bash
curl -X POST http://localhost:3000/analyze \
  -H "Content-Type: application/json" \
  -d '{"text": "What is ai?"}'
```

This command sends your query to the agent, which looks up the information in its ChromaDB RAG. ![](https://i.imgur.com/qspLodM.png)

It follows four clear stages: classification (deciding question vs. general), knowledge retrieval or tool execution, reasoning (building a step-by-step answer plan), and response generation (preparing a helpful final reply).

 Sample queries to try -

- "Tell me about Helicone."
- "What is RAG?"
- "What is the Model Context Protocol?"
- "What time is it?"

## Monitor Your AI Agent with Helicone

Now that your sentiment analysis agent is running, let's add observability so you precisely understand how it works behind the scenes.

We're using Helicone as our observability layer. It acts as a proxy between your AI agent and Groq's API, capturing every request and response. This gives you a holistic picture of how your AI agent is performing without changing your app's core logic.

By sending requests to Groq through the Helicone proxy, we automatically have access to -

* Full request/response logging
* Latency metrics
* Usage tracking
* Sessions - a group of related user interactions for debugging and analysis

You've already set up the Helicone proxy when initializing Groq API connections in your `main.ts` code.

```ts
const groq = new Groq({
  apiKey: process.env.GROQ_API_KEY ?? "",
  baseURL: "https://groq.helicone.ai",
  defaultHeaders: {
    "Helicone-Auth": `Bearer ${process.env.HELICONE_API_KEY ?? ""}`,
  },
});
```

This setup routes all traffic through Helicone, enabling robust monitoring with zero additional code.

Once you send a few test requeststo the agent, visit your Helicone dashboard, and you'll see your AI usage details like this -

![](https://i.imgur.com/0RIFOGz.png)

Navigate to the Sessions menu from the left pane, and there you can click into a session to view -

* View raw request and response data
* Inspect latency and token usage
* Compare different inputs side-by-side
*  Test out prompts

![](https://i.imgur.com/TJqfUCw.png)

Our simple agent has a four-step workflow that is fully traceable in Helicone sessions. We logged each step under its own session path. 

![](https://i.imgur.com/YZgkGW8.png)
Our agent also makes tool calls to retrieve information and perform actions as needed. Helicone provides visibility into these calls, showing exactly how your agent is leveraging external capabilities. 
![](https://i.imgur.com/jVLqbpP.png)

In our implementation, we've successfully integrated Helicone's manual logging for the `getCurrentTime` tool, which provides the current timestamp. This practice lets you follow the complete decision-making process behind every agent's reply to help spot issues like wrong classifications or missing knowledge.

## Get Started with Helicone

Ready to add enterprise-grade observability to your AI agents? Get started with Helicone today -

* Sign up for a free Helicone account at helicone.ai
* Get your API key from the Helicone dashboard
* Update your API clients to route through Helicone (for Groq, OpenAI, Anthropic, etc.)
* Add session tracking to follow conversations
* Implement tool logging using the patterns shown above

Helicone offers a generous free tier that's perfect for getting started, with paid plans available as your demands grow.

## Conclusion

Observability is key to building reliable AI agents. Your AI app can't succeed without visibility into how your agent behaves, what it sees, how it responds, and where it might fail.

In this tutorial, we built a multi-step AI agent exposed through a simple REST API. The agent uses Groq as the LLM backend, ChromaDB for lightweight RAG-style knowledge retrieval, and Helicone for deep observability into every step of the workflow.

With Helicone sessions, you can now clearly track how each query moves through classification, retrieval, reasoning, and response generation — giving you full visibility into your AI agent's behavior and performance.

#### Frequently asked questions

* How does Helicone track multi-step flows in AI agents?

	Helicone uses session identifiers and custom property headers to trace each step, allowing you to view a request's entire lifecycle, from classification to retrieval to final response, within a single, searchable session in the Helicone dashboard.

* Can I use Helicone with other LLM providers (like OpenAI or Anthropic)?

	Yes, Helicone supports multiple LLM backends, including OpenAI, Anthropic, and Groq. You only need to change the baseURL and API key used in your SDK setup.

* How can I debug specific steps like tool calls or RAG retrievals?

	Helicone tags each execution of tools or knowledge retrieval steps using its own `Helicone-Session-Path` and custom properties. You can filter by these to drill down into individual tool calls, see input/output data, and troubleshoot issues like wrong classifications or missing knowledge.


