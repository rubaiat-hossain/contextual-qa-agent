# Monitoring AI Agents with Helicone

As AI agents become more common in apps and services, it's essential to understand how they behave. In this guide, we'll build a multi-step qa analysis agent powered by Groq for fast LLM responses and wrap it with a REST API so you can easily use it using tools like Curl. Then, we'll integrate Helicone to monitor and debug the agent in real-time, giving you a clear view of how it performs and responds.

## Setting Up the QA Analysis Agent

Our simple AI agent can answer questions about specific topics by retrieving information from its RAG knowledge base, or handle general statements with contextual awareness. It also routes all LLM traffic through Helicone, which acts as a proxy and observability layer, letting us track requests, analyze metrics latencies, debug sessions, and more.

You can get the full working agent at this [GitHub repository](https://github.com/rubaiat-hossain/contextual-qa-agent).

### Prerequisites

To follow along, you‚Äôll need the following tools and API keys configured.

* Node.js v18+
* Npm - installed and running
* ChromaDB - running on port 8000
* Groq API key ‚Äì https://console.groq.com
* Helicone API key ‚Äì https://www.helicone.ai

### Step 1. Initialize the project

Begin by creating a new directory for your project and initializing it using the following commands.

```bash
mkdir qa-agent-tutorial
cd qa-agent-tutorial
npm init -y
```

### Step 2. Install the dependencies

You need to install the necessary packages for our AI agent using these commands.

```bash
npm install express dotenv zod chromadb groq-sdk @helicone/helpers
npm install -D typescript tsx @types/node @types/express
```

### Step 3. Configure TypeScript

Initialize TypeScript configuration using -

```bash
npx tsc --init
```

### Step 4. Create your agent

Create a `main.ts` file and add the following code to it.

```ts
import { config } from "dotenv";
config();

import express, { Request, Response } from "express";
import Groq from "groq-sdk";
import { randomUUID } from "crypto";
// @ts-ignore
import { ChromaClient } from "chromadb";
import { HeliconeManualLogger } from "@helicone/helpers";

const app = express();
app.use(express.json());

const chroma = new ChromaClient({ path: "http://localhost:8000" });
let collection: Awaited<ReturnType<ChromaClient["getOrCreateCollection"]>>;

const groq = new Groq({
  apiKey: process.env.GROQ_API_KEY ?? "",
  baseURL: "https://groq.helicone.ai",
  defaultHeaders: {
    "Helicone-Auth": `Bearer ${process.env.HELICONE_API_KEY ?? ""}`,
  },
});

const heliconeLogger = new HeliconeManualLogger({
  apiKey: process.env.HELICONE_API_KEY,
});

// FIXED: Reimplemented based on OpenAI example
async function getCurrentTime(sessionId: string, sessionName: string): Promise<string> {
  try {
    const timestamp = await heliconeLogger.logRequest(
      {
        _type: "tool",
        toolName: "getCurrentTime",
        input: {},
      },
      async (recorder) => {
        const now = new Date().toISOString();
        // Simply record the result
        recorder.appendResults({ time: now });
        // Return the raw timestamp
        return now;
      },
      {
        "Helicone-Session-Id": sessionId,
        "Helicone-Session-Path": "/manual-tool-log",
        "Helicone-Session-Name": sessionName,
      }
    );
    
    console.log(`‚úÖ getCurrentTime executed: ${timestamp}`);
    return timestamp;
  } catch (error) {
    console.error("Error in getCurrentTime tool:", error);
    throw error;
  }
}

// FIXED: Reimplemented based on OpenAI example
async function retrieveFromChromaDB(query: string, sessionId: string, sessionName: string): Promise<string> {
  try {
    const document = await heliconeLogger.logRequest(
      {
        _type: "tool",
        toolName: "ChromaDBRetrieval",
        input: { query },
      },
      async (recorder) => {
        // Query ChromaDB
        const results = await collection.query({
          queryTexts: [query],
          nResults: 1,
        });
        
        const retrievedDoc = results.documents?.[0]?.[0] ?? "No relevant knowledge found.";
        
        // Record the result 
        recorder.appendResults({ document: retrievedDoc });
        
        // Return the raw document
        return retrievedDoc;
      },
      {
        "Helicone-Session-Id": sessionId,
        "Helicone-Session-Path": "/knowledge-retrieval",
        "Helicone-Session-Name": sessionName,
      }
    );
    
    console.log(`‚úÖ ChromaDBRetrieval executed`);
    return document;
  } catch (error) {
    console.error("Error in ChromaDBRetrieval tool:", error);
    throw error;
  }
}

async function seedKnowledgeBase() {
  collection = await chroma.getOrCreateCollection({ name: "knowledge" });

  const count = await collection.count();
  if (count === 0) {
    console.log("ÔøΩÔøΩ Seeding ChromaDB knowledge base...");
    await collection.add({
      ids: ["ai", "helicone", "rag", "observability", "mcp", "llmops"],
      metadatas: [
        { topic: "AI" },
        { topic: "Helicone" },
        { topic: "RAG" },
        { topic: "Observability" },
        { topic: "Model Context Protocol" },
        { topic: "LLMOps" },
      ],
      documents: [
        "Artificial Intelligence (AI) is the simulation of human-like intelligence by machines to perform tasks like learning, reasoning, and problem-solving.",
        "Helicone is a developer platform for monitoring and debugging AI agents and LLM applications. It provides observability into prompts, costs, latency, and session flows.",
        "Retrieval-Augmented Generation (RAG) is an AI framework that retrieves external information and injects it into the prompt for better, more factual responses.",
        "Observability in AI refers to the practice of understanding, monitoring, and debugging model behavior by tracking key metrics like latency, cost, token usage, and session traces.",
        "Model Context Protocol (MCP) allows AI agents to connect external tools, APIs, or services dynamically at runtime using a standard protocol for enhanced capabilities.",
        "LLMOps refers to operational practices around managing, monitoring, scaling, and debugging large language models in production environments.",
      ],
    });
    console.log("‚úÖ Knowledge base seeded.");
  }
}

async function processMultiStepQuery(text: string): Promise<string> {
  const sessionId = randomUUID();
  const sessionName = "Multi-Step RAG Agent";

  console.log(`üöÄ New Query: "${text}"`);
  console.log(`üìä Session ID: ${sessionId}`);

  const isTimeQuestion = /\b(what\s+time\s+is\s+it|current\s+time|time\s+now)\b/i.test(text);

  let context = "";

  if (isTimeQuestion) {
    console.log("‚è∞ Processing time question...");
    // FIXED: Get the raw timestamp
    const timestamp = await getCurrentTime(sessionId, sessionName);
    context = `Current time is ${timestamp}`;    
  } else {
    console.log("ü§î Classifying query...");
    const classify = await groq.chat.completions.create(
      {
        messages: [
          { role: "system", content: "Respond ONLY with 'question' or 'general'." },
          { role: "user", content: `Classify: "${text}"` },
        ],
        model: "meta-llama/llama-4-scout-17b-16e-instruct",
        temperature: 0.3,
        max_tokens: 10,
      },
      {
        headers: {
          "Helicone-Session-Id": sessionId,
          "Helicone-Session-Path": "/classify",
          "Helicone-Session-Name": sessionName,
        },
      }
    );

    const classification = classify.choices?.[0]?.message?.content?.trim().toLowerCase() ?? "general";
    console.log(`üìä Classification: ${classification}`);

    if (classification.includes("question")) {
      console.log("üîç Retrieving knowledge...");
      // FIXED: Get the raw document
      context = await retrieveFromChromaDB(text, sessionId, sessionName);
    } else {
      console.log("‚è∞ General query, providing time...");
      // FIXED: Get the raw timestamp
      const timestamp = await getCurrentTime(sessionId, sessionName);
      context = `Current time is ${timestamp}`;
    }
  }

  console.log("üß† Starting reasoning step...");
  const reasoning = await groq.chat.completions.create(
    {
      messages: [
        { role: "system", content: "You are a reasoning assistant." },
        { role: "user", content: `Context: "${context}". Reason about how to answer "${text}".` },
      ],
      model: "meta-llama/llama-4-scout-17b-16e-instruct",
      temperature: 0.3,
      max_tokens: 200,
    },
    {
      headers: {
        "Helicone-Session-Id": sessionId,
        "Helicone-Session-Path": "/reasoning",
        "Helicone-Session-Name": sessionName,
      },
    }
  );

  const reasoningOutput = reasoning.choices?.[0]?.message?.content ?? "";
  console.log(`üß† Reasoning complete`);

  console.log("üéØ Generating final response...");
  const finalReply = await groq.chat.completions.create(
    {
      messages: [
        {
          role: "system",
          content: `Friendly assistant.\nContext: "${context}".\nReasoning: "${reasoningOutput}".`,
        },
        { role: "user", content: text },
      ],
      model: "meta-llama/llama-4-scout-17b-16e-instruct",
      temperature: 0.5,
      max_tokens: 500,
    },
    {
      headers: {
        "Helicone-Session-Id": sessionId,
        "Helicone-Session-Path": "/final-response",
        "Helicone-Session-Name": sessionName,
      },
    }
  );

  return finalReply.choices?.[0]?.message?.content ?? "No final response.";
}

app.post("/analyze", async (req: Request, res: Response) => {
  const { text } = req.body;
  if (!text) {
    res.status(400).json({ error: "Missing text" });
    return;
  }

  try {
    console.log(`üì• Received query: "${text}"`);
    const result = await processMultiStepQuery(text);
    console.log(`üì§ Sending response`);
    res.json({ response: result });
  } catch (err: unknown) {
    console.error("‚ùå API Error:", err);
    res.status(500).json({ error: err instanceof Error ? err.message : "Internal Server Error" });
  }
});

app.get("/health", (_req: Request, res: Response) => {
  res.json({ status: "ok" });
});

const PORT = process.env.PORT || 3000;
seedKnowledgeBase().then(() => {
  app.listen(PORT, () => {
    console.log(`‚úÖ Server running at http://localhost:${PORT}`);
    console.log(`POST /analyze {"text": "your question"}`);
  });
});
```

### Step 5. Configure environment

Create a `.env` file and add your API keys.

```bash
GROQ_API_KEY=your_groq_api_key
HELICONE_API_KEY=your_helicone_api_key
```
You also need to start ChromaDB.

```bash
docker run -p 8000:8000 ghcr.io/chroma-core/chroma:latest
```

### Step 6. Run the agent

With your environment variables configured, you can now start the agent.

```bash
npx tsx main.ts
```

If you've set up the environment correctly, you'll see -

`‚úÖ Server running at http://localhost:3000`

### Step 7. Test the agent

Once our AI agent is running, it exposes a REST endpoint that you can use to send queries and inspect its behavior.

You can test the agent using `curl` -

```bash
curl -X POST http://localhost:3000/analyze \
  -H "Content-Type: application/json" \
  -d '{"text": "What is ai?"}'
```

This command sends your query to the agent, which looks up the information in its ChromaDB RAG. ![](https://i.imgur.com/qspLodM.png)

It follows four clear stages: classification (deciding question vs. general), knowledge retrieval or tool execution, reasoning (building a step-by-step answer plan), and response generation (preparing a helpful final reply).

 Sample queries to try -

- "Tell me about Helicone."
- "What is RAG?"
- "What is the Model Context Protocol?"
- "What time is it?"

## Monitor Your AI Agent with Helicone

Now that your sentiment analysis agent is running, let's add observability so you precisely understand how it works behind the scenes.

We're using Helicone as our observability layer. It acts as a proxy between your AI agent and Groq's API, capturing every request and response. This gives you a holistic picture of how your AI agent is performing without changing your app's core logic.

By sending requests to Groq through the Helicone proxy, we automatically have access to -

* Full request/response logging
* Latency metrics
* Usage tracking
* Sessions - a group of related user interactions for debugging and analysis

You've already set up the Helicone proxy when initializing Groq API connections in your `main.ts` code.

```ts
const groq = new Groq({
  apiKey: process.env.GROQ_API_KEY ?? "",
  baseURL: "https://groq.helicone.ai",
  defaultHeaders: {
    "Helicone-Auth": `Bearer ${process.env.HELICONE_API_KEY ?? ""}`,
  },
});
```

This setup routes all traffic through Helicone, enabling robust monitoring with zero additional code.

Once you send a few test requeststo the agent, visit your Helicone dashboard, and you'll see your AI usage details like this -

![](https://i.imgur.com/0RIFOGz.png)

Navigate to the Sessions menu from the left pane, and there you can click into a session to view -

* View raw request and response data
* Inspect latency and token usage
* Compare different inputs side-by-side
*  Test out prompts

![](https://i.imgur.com/TJqfUCw.png)

Our simple agent has a four-step workflow that is fully traceable in Helicone sessions. We logged each step under its own session path. 

![](https://i.imgur.com/YZgkGW8.png)
Our agent also makes tool calls to retrieve information and perform actions as needed. Helicone provides visibility into these calls, showing exactly how your agent is leveraging external capabilities.

![](https://i.imgur.com/mOpyv4C.png)

In our implementation, we've successfully integrated Helicone's manual logging for the `getCurrentTime` tool, which provides the current timestamp. This practice lets you follow the complete decision-making process behind every agent's reply to help spot any issues (like wrong classifications or missing knowledge).

## Get Started with Helicone

Ready to add enterprise-grade observability to your AI agents? Get started with Helicone today -

* Sign up for a free Helicone account at helicone.ai
* Get your API key from the Helicone dashboard
* Update your API clients to route through Helicone (for Groq, OpenAI, Anthropic, etc.)
* Add session tracking to follow conversations
* Implement tool logging using the patterns shown above

Helicone offers a generous free tier that's perfect for getting started, with paid plans available as your demands grow.

## Conclusion

Observability is key to building reliable AI agents. Your AI app can't succeed without visibility into how your agent behaves, what it sees, how it responds, and where it might fail.

In this tutorial, we built a multi-step AI agent exposed through a simple REST API. The agent uses Groq as the LLM backend, ChromaDB for lightweight RAG-style knowledge retrieval, and Helicone for deep observability into every step of the workflow.

With Helicone sessions, you can now clearly track how each query moves through classification, retrieval, reasoning, and response generation ‚Äî giving you full visibility into your AI agent's behavior and performance.
